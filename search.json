[
  {
    "objectID": "01数据处理.html",
    "href": "01数据处理.html",
    "title": "数据处理",
    "section": "",
    "text": "import numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import dataset, sampler, dataloader"
  },
  {
    "objectID": "01数据处理.html#dataset",
    "href": "01数据处理.html#dataset",
    "title": "数据处理",
    "section": "Dataset",
    "text": "Dataset\n\n数据源，数据的集合\n我们必须知道数据源的大小(len)，如何获得每个数据源的元素(getitem)\n初始化过程中，一般会加载整个数据集合。\n__getitem(index)__的index应该理解为key,Dataset可以理解成一个map\n\n\nclass MyDataSet(dataset.Dataset):\n    def __init__(self,n=100) :\n        self.data=list(range(n))\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, index):\n        return self.data[index]\n\n我们定义上面的数据源，然后查看元素10,20\n\nds=MyDataSet()\nprint(f\"ds[10]={ds[10]},ds[20]={ds[20]}\")\n\nds[10]=10,ds[20]=20\n\n\n\nDataSet example\n我们创建一个Paddy数据的Dataset类,Paddy数的目录结构是：basepath/label/image.jpg\n\nfrom fastai.vision.all import *\nclass PaddyDataSet(dataset.Dataset):\n    def __init__(self,basepath='') :\n        self.data=get_image_files(basepath)\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, index):\n        filepath=self.data[index]\n        x=Image.open(filepath)\n        y=parent_label(filepath)\n        return x,y\n\n创建一个数据集PaddyDataSet\n\nds=PaddyDataSet('/Users/zhanggxk/project/paddy/data/train_images')\nitem=ds[10]\nprint(f'ds size={len(ds)},item={item}')\nitem[0].to_thumb(100,)\n\nds size=10407,item=(&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640&gt;, 'dead_heart')"
  },
  {
    "objectID": "01数据处理.html#dataloader对象",
    "href": "01数据处理.html#dataloader对象",
    "title": "数据处理",
    "section": "DataLoader对象",
    "text": "DataLoader对象\nDataLoader用于批量组合 Dataset的item,并把item转换成为tensor,准备输入给model\n\n迭代一个DataLoader,会触发batch_size次调用 Dateset的getitem方法。\nShuffle Dataset的顺序\n在调用Dataset.getitem的时候， 使用的key是int类型，如果dataset的key是其他类型，需要传递自定义sampler\nshuffle参数是对key是int类型的数据集就行随机打乱的选项，自定义sampler 不能使用这个参数。\ncollate_fn定义了如何把 原始的batah_data(list of items) 进一步 转换\n\n\ndata_loader=dataloader.DataLoader(\n    MyDataSet(),\n    batch_size=5,\n    shuffle=False,\n    drop_last=False,\n)\nprint(f\"size of loader {len(data_loader)}\")\nprint(f\"one batch : {next(iter(data_loader))}\")\n\nsize of loader 20\none batch : tensor([0, 1, 2, 3, 4])\n\n\n\nPaddy DataLoader example\n\ndef collate_fn(batch_data):\n    '''\n        batch_data:[(img1,lb1),(img2,lb2),(img3,lb3)...]\n    '''\n    from torchvision import transforms as tfms\n    toTensor=tfms.ToTensor()\n\n    xs=[ toTensor(item[0].resize((32,32)) ) for item in batch_data ]\n    ys=torch.tensor([ 1 for item in batch_data])\n\n    return torch.stack(xs),ys\n\ndata_loader=dataloader.DataLoader(\n    PaddyDataSet('/Users/zhanggxk/project/paddy/data/train_images'),\n    batch_size=4,\n    shuffle=False,\n    drop_last=False,\n    collate_fn=collate_fn\n)\nprint(f\"size of loader {len(data_loader)}\")\nbatch=next(iter(data_loader))\n\nprint(f\"one batch : {batch[0].shape},{batch[1]}\")\n\nsize of loader 2602\none batch : torch.Size([4, 3, 32, 32]),tensor([1, 1, 1, 1])"
  },
  {
    "objectID": "01数据处理.html#sampler",
    "href": "01数据处理.html#sampler",
    "title": "数据处理",
    "section": "Sampler",
    "text": "Sampler\n采样器的作用是 从Dataset中，为一个batch 返回 索引。\n\n返回的索引的元素需要__get_item(index)__的index类型兼容。\nSampler也要知道Dataset的大小，这样才能确定返回索引的范围\n\n\nclass MyRandomSampler(sampler.Sampler):\n    def __init__(self,ds_size=100):\n        self.ds_size=ds_size\n    def __iter__(self):\n        return iter(np.random.permutation(range(self.ds_size)))\n\nfor x in iter(MyRandomSampler(10)):\n    print(x,end=' ')\n\n3 4 5 9 1 8 0 7 2 6 \n\n\n下面的demo做了一个对比，没有加入MySampler的data_loader返回的batch是固定顺序的，加入MySampler的data_loader返回的batch是随机的排列。\n\ndata_loader=dataloader.DataLoader(\n    MyDataSet(n=15),\n    batch_size=3,\n    drop_last=False,\n)\n\nfor x in data_loader:\n    print(x,end=' ')\n\n\ndata_loader=dataloader.DataLoader(\n    MyDataSet(n=15),\n    batch_size=3,\n    drop_last=False,\n    sampler=MyRandomSampler(15),\n)\nprint()\nfor x in data_loader:\n    print(x,end=' ')\n\ntensor([0, 1, 2]) tensor([3, 4, 5]) tensor([6, 7, 8]) tensor([ 9, 10, 11]) tensor([12, 13, 14]) \ntensor([12,  8,  9]) tensor([7, 4, 2]) tensor([14,  1, 11]) tensor([0, 5, 6]) tensor([ 3, 10, 13]) \n\n\n\n与DataSet index兼容的 Sampler\n下面的例子中：\n\nTupleDataSet key=tutle(),value=key中索引对于的值\nTupleSampler 的迭代器返回2个key,分别是(i,i+1)\ncollate_fn是整理函数，输入是 MyDataset的value,这里选择原样返回！\n\n\nclass TupleDataSet(dataset.Dataset):\n    def __init__(self,n=100) :\n        self.data=list(range(n))\n    def __getitem__(self, key):   \n        v=self.data  \n        return v[key[0]],v[key[1]]\n    def __len__(self):\n        return 10\nds=TupleDataSet(10)\nprint(f\"ds[(2,3)]={ds[(2,3)]}\")\n\nds[(2,3)]=(2, 3)\n\n\n\nclass TupleSampler(sampler.Sampler):\n    def __init__(self,ds_size):\n        self.ds_size=ds_size\n    def __iter__(self):\n        r=[(i,(i+1)%self.ds_size) for i in range(self.ds_size)]\n        return iter(r)\n    \ndata_loader=dataloader.DataLoader(\n    TupleDataSet(10),\n    batch_size=3,\n    shuffle=False,\n    drop_last=False,\n    sampler=TupleSampler(10),\n    collate_fn=lambda x:x\n)\nfor x in data_loader:\n    print(x)\n\n[(0, 1), (1, 2), (2, 3)]\n[(3, 4), (4, 5), (5, 6)]\n[(6, 7), (7, 8), (8, 9)]\n[(9, 0)]"
  },
  {
    "objectID": "01数据处理.html#分布式数据组件",
    "href": "01数据处理.html#分布式数据组件",
    "title": "数据处理",
    "section": "分布式数据组件",
    "text": "分布式数据组件\n\nDDP(Distribute Data Parallet)\n\n每个GPU上都有相同的模型\n数据分散到不同的GPU上，进行forward,backward\n使用Ring All Reduce 算法，同步不同GPU上的grad\n更新grad\n\n\n\n\n设备\n权重\n\n\n\n\nG0\nw0\n\n\nG1\nw1\n\n\nG2\nw2\n\n\nG3\nw3\n\n\n\nG0-&gt;G1-&gt;G2-&gt;G3-&gt;G0\n第一轮同步 |设备| 权重 | | - | - | | G0| w3+w0 | | G1| w0+w1 | | G2| w1+w2 | | G3| w2+w3 |\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001)\n\n\n    xs=torch.randn(20, 10)\n    labels = torch.randn(20, 5).to(rank)\n\n    for i in range(200):\n        optimizer.zero_grad()\n        outputs = ddp_model(xs)\n        loss_fn(outputs, labels).backward()\n        optimizer.step()\n\n    cleanup()\n\n\ndef run_demo(world_size):\n    mp.spawn(demo_basic,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)"
  },
  {
    "objectID": "transformer.html",
    "href": "transformer.html",
    "title": "Transformer101",
    "section": "",
    "text": "Tokenizer用于把str转换成对应的索引,也可以把索引转换回来。\n\n1.创建一个bert的分词器，\n\ntokenizer=AutoTokenizer.from_pretrained('prajjwal1/bert-medium')\n\n2.定义我们需要进行分词的tokens.\n\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n    \"I love her\"\n]\n\n3.使用padding保证返回的长度一致,并设置返回数据类型为torch.tensor\n\ninputs=tokenizer(raw_inputs,padding=True,return_tensors='pt')\nprint(tokenizer.decode(inputs['input_ids'][0]))\nprint(tokenizer.decode(inputs['input_ids'][1]))\nprint(tokenizer.decode(inputs['input_ids'][2]))\n\n[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n[CLS] i hate this so much! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n[CLS] i love her [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\n\n4.有时需要使用max_length,truncation参数，指定分词串的最大长度。\n\ninputs=tokenizer(raw_inputs,padding=True,max_length=8,truncation=True,return_tensors='pt')\nprint(tokenizer.decode(inputs['input_ids'][0]))\nprint(tokenizer.decode(inputs['input_ids'][1]))\nprint(tokenizer.decode(inputs['input_ids'][2]))\n\n[CLS] i've been waiting for [SEP]\n[CLS] i hate this so much! [SEP]\n[CLS] i love her [SEP] [PAD] [PAD] [PAD]\n\n\n5.打印分词结果，可以看到bert tokenizer还返回token_type_ids,attention_mask\n\nfor x in inputs.items():\n    print(f\"{x[0]}:{x[1].shape}\")\n\ninput_ids:torch.Size([3, 16])\ntoken_type_ids:torch.Size([3, 16])\nattention_mask:torch.Size([3, 16])\n\n\n\n\n\n把2个tokens数组 分别作为参数传递给tokenizer，即可对这两句子连接并转换成索引,句子之间会自动加入[SEP]。\n\n\nraw_inputs1 = [\n    \"I love her\"\n]\nraw_inputs2 = [\n    \"I hate this so much!\",\n]\ninputs=tokenizer(raw_inputs1,raw_inputs2,padding=True,return_tensors='pt')\nprint(tokenizer.decode(inputs['input_ids'][0]))\n\n[CLS] i love her [SEP] i hate this so much! [SEP]\n\n\n\n\n\n\nprint(len(tokenizer.get_vocab()))\nprint(tokenizer.pad_token_id,tokenizer.pad_token)\nprint(tokenizer.sep_token_id,tokenizer.sep_token)\nprint(tokenizer.eos_token_id,tokenizer.eos_token)\n\nUsing eos_token, but it is not set yet.\n\n\n30522\n0 [PAD]\n102 [SEP]\nNone None"
  },
  {
    "objectID": "transformer.html#tokenizer连接句子",
    "href": "transformer.html#tokenizer连接句子",
    "title": "Transformer101",
    "section": "",
    "text": "把2个tokens数组 分别作为参数传递给tokenizer，即可对这两句子连接并转换成索引,句子之间会自动加入[SEP]。\n\n\nraw_inputs1 = [\n    \"I love her\"\n]\nraw_inputs2 = [\n    \"I hate this so much!\",\n]\ninputs=tokenizer(raw_inputs1,raw_inputs2,padding=True,return_tensors='pt')\nprint(tokenizer.decode(inputs['input_ids'][0]))\n\n[CLS] i love her [SEP] i hate this so much! [SEP]"
  },
  {
    "objectID": "transformer.html#tokenizer的重要属性",
    "href": "transformer.html#tokenizer的重要属性",
    "title": "Transformer101",
    "section": "",
    "text": "print(len(tokenizer.get_vocab()))\nprint(tokenizer.pad_token_id,tokenizer.pad_token)\nprint(tokenizer.sep_token_id,tokenizer.sep_token)\nprint(tokenizer.eos_token_id,tokenizer.eos_token)\n\nUsing eos_token, but it is not set yet.\n\n\n30522\n0 [PAD]\n102 [SEP]\nNone None"
  },
  {
    "objectID": "transformer.html#基础模型automodel",
    "href": "transformer.html#基础模型automodel",
    "title": "Transformer101",
    "section": "基础模型：AutoModel",
    "text": "基础模型：AutoModel\n\n# model=AutoModel.from_pretrained('prajjwal1/bert-medium')\nmodel=AutoModel.from_pretrained('prajjwal1/bert-medium',ignore_mismatched_sizes=True)\n\n使用model(**inputs)对输入进行编码\n\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n    \"I love her\"\n]\ninputs=tokenizer(raw_inputs,padding=True,return_tensors='pt')\noutput=model(**inputs)\noutput.last_hidden_state.shape\n\ntorch.Size([3, 16, 512])\n\n\n和pytorch的model一样，可以使用get_submodule查看模型的结构\n\nmodel.get_submodule(\"pooler\")\n\nBertPooler(\n  (dense): Linear(in_features=512, out_features=512, bias=True)\n  (activation): Tanh()\n)"
  },
  {
    "objectID": "transformer.html#分类模型automodelforsequenceclassification",
    "href": "transformer.html#分类模型automodelforsequenceclassification",
    "title": "Transformer101",
    "section": "分类模型：AutoModelForSequenceClassification",
    "text": "分类模型：AutoModelForSequenceClassification\n\nmodel=AutoModelForSequenceClassification.from_pretrained(\n    'prajjwal1/bert-medium',                                                     \n     num_labels=33,  #覆盖classifer header\n     ignore_mismatched_sizes=True)\n\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmodel.get_submodule('classifier')\n\nLinear(in_features=512, out_features=33, bias=True)"
  },
  {
    "objectID": "transformer.html#生成模型automodelforcausallm",
    "href": "transformer.html#生成模型automodelforcausallm",
    "title": "Transformer101",
    "section": "生成模型:AutoModelForCausalLM",
    "text": "生成模型:AutoModelForCausalLM\n\nmodel=AutoModelForCausalLM.from_pretrained('prajjwal1/bert-medium',ignore_mismatched_sizes=True)\nmodel.get_submodule('cls')\n\nIf you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nBertOnlyMLMHead(\n  (predictions): BertLMPredictionHead(\n    (transform): BertPredictionHeadTransform(\n      (dense): Linear(in_features=512, out_features=512, bias=True)\n      (transform_act_fn): GELUActivation()\n      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n    )\n    (decoder): Linear(in_features=512, out_features=30522, bias=True)\n  )\n)"
  },
  {
    "objectID": "transformer.html#模型的保存与加载",
    "href": "transformer.html#模型的保存与加载",
    "title": "Transformer101",
    "section": "模型的保存与加载",
    "text": "模型的保存与加载\n\nmodel.save_pretrained('zxk/my-bert')\nmodel=AutoModel.from_pretrained('zxk/my-bert')"
  },
  {
    "objectID": "transformer.html#数据集的下载浏览",
    "href": "transformer.html#数据集的下载浏览",
    "title": "Transformer101",
    "section": "数据集的下载，浏览",
    "text": "数据集的下载，浏览\n\n# glue是由10多种数据集(cola,sst2,mrpc,qqp...)构成的benckmark\n# mrpc是数据集，比较2句话是否是一个意思\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\nraw_datasets\n\nFound cached dataset glue (/Users/zhanggxk/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n\n\n查看数据集，我们查看训练数据集，索引为1的数据\n\nraw_datasets['train'][1:2]\n# 理解数据\n# s1:在1998年，Yucaipa公司拥有Dominick's超市连锁店，\n# 随后以25亿美元的价格将该连锁店出售给Safeway公司。\n\n# s2:1995年Yucaipa以6.93亿美元的价格购买了Dominick's超市连锁店，\n# 然后在1998年以18亿美元的价格将其出售给了Safeway公司。\n# label:0，两句话无关\n# idx:1\n\n{'sentence1': [\"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\"],\n 'sentence2': [\"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\"],\n 'label': [0],\n 'idx': [1]}\n\n\n查看数据的特征\n\nraw_datasets['train'].features\n\n{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}"
  },
  {
    "objectID": "transformer.html#向量化tensor",
    "href": "transformer.html#向量化tensor",
    "title": "Transformer101",
    "section": "向量化(tensor)",
    "text": "向量化(tensor)\n\ndataset.map方法结合tokenizer，可以把数据集合转换成tensor，输入到model。\n\n\ndef tokenizr_func(example):\n    return tokenizer(example['sentence1'],example['sentence2'],\n                   padding=True,\n                   return_tensors='pt',\n                   truncation=True,\n                   max_length=128)\n\nds=raw_datasets.map(tokenizr_func,batched=True)\n\n\n\n\n\n\n\n\n\n\n删除无用的列，重命名labels的原因是 ,transformer认为labels的特征是目标.\n\nds=ds.remove_columns(['sentence1','sentence2','idx']).rename_column('label','labels')\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1725\n    })\n})"
  },
  {
    "objectID": "transformer.html#分割数据集",
    "href": "transformer.html#分割数据集",
    "title": "Transformer101",
    "section": "分割数据集",
    "text": "分割数据集\n\nraw_datasets['train'].train_test_split(0.5)\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1834\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1834\n    })\n})"
  },
  {
    "objectID": "transformer.html#从pandas获得dataset",
    "href": "transformer.html#从pandas获得dataset",
    "title": "Transformer101",
    "section": "从pandas获得dataset",
    "text": "从pandas获得dataset\n\ndf=pd.read_csv('model_benchmark.csv')\nds=Dataset.from_pandas(df)\nds.features\n\n{'model_name': Value(dtype='string', id=None),\n 'learning_rate': Value(dtype='float64', id=None),\n 'pool': Value(dtype='string', id=None),\n 'dataset': Value(dtype='string', id=None),\n 'GPU_mem': Value(dtype='float64', id=None),\n 'error_rate': Value(dtype='float64', id=None),\n 'valid_loss': Value(dtype='float64', id=None),\n 'train_loss': Value(dtype='float64', id=None),\n 'fit_time': Value(dtype='float64', id=None)}"
  },
  {
    "objectID": "transformer.html#动态与静态padding",
    "href": "transformer.html#动态与静态padding",
    "title": "Transformer101",
    "section": "动态与静态padding",
    "text": "动态与静态padding\n\nsource\n\nbatch_dataset\n\n batch_dataset (ds, mapping_func)\n\n使用mapping_func作用于数据集ds上，把转换后的结果返回\n\n\n\n\nDetails\n\n\n\n\nds\n数据集\n\n\nmapping_func\nfunc(example,maxlen)签名的函数\n\n\n\n\n\n静态padding\n\ndef padding_fix(example,maxlen=512):\n    \"\"\"\n    把example的句子通过`padding`填充的方式，转换成固定`maxlen`长度的tokens，然后返回。\n    \"\"\"\n    ret= tokenizer(example['sentence1'],\n                   example['sentence2'],\n                   padding=True,\n                   truncation=True,\n                   max_length=maxlen)\n    return ret\n\n用padding_fix处理好的数据集，每个batch,字符序列的长度都固定\n\nds_train=batch_dataset(raw_datasets['train'],padding_fix)\ndls=dataloader.DataLoader(ds_train,batch_size=50,shuffle=False)\n\nfor k,t in enumerate(dls):\n    print(t['input_ids'].shape)\n    if k==2:break\n\n### 动态padding\n\n\n\n\ntorch.Size([50, 89])\ntorch.Size([50, 89])\ntorch.Size([50, 89])\n\n\n\n\n动态padding\n\ndef padding_dynamic(example,maxlen=512):\n    \"\"\"\n    按照example的句子按照句子的实际长度进行返回。\n    \"\"\"\n    ret= tokenizer(example['sentence1'],example['sentence2'])\n    return ret\n\n通过传递给DataLoader 对象收集器 DataCollatorWithPadding，每一个batch的长度都是动态被填充为batch中最长序列的。\n\nds_train=batch_dataset(raw_datasets['train'],padding_dynamic)\n\n#数据收集器\ndata_collector=DataCollatorWithPadding(tokenizer=tokenizer)\ndls=dataloader.DataLoader(ds_train,batch_size=5,shuffle=False,collate_fn=data_collector)\n\nfor k,t in enumerate(dls):\n    print(t['input_ids'].shape)\n    if k==2:break\n\n\n\n\ntorch.Size([5, 67])\ntorch.Size([5, 62])\ntorch.Size([5, 61])\n\n\n\n\nDataCollatorWithPadding\n\ndata_collator接受一个map，把map中的list元素转换成tensor。如果list中的tensor长度不一，那么就会padding补齐。\n\n\n# data_collator接受一个map，都没个把每个value(list)都转换成对于的tensor,如果value的元素长度参差不齐时，使用padding补齐后，在转成tensor\ninp={\n    'input_ids':[[1],[2,2],[3,3,3]]\n}\ndata_collector=DataCollatorWithPadding(tokenizer=tokenizer)\nbatch=data_collector(inp)\nbatch['input_ids'],batch['attention_mask']\n\n(tensor([[1, 0, 0],\n         [2, 2, 0],\n         [3, 3, 3]]),\n tensor([[1, 0, 0],\n         [1, 1, 0],\n         [1, 1, 1]]))"
  },
  {
    "objectID": "transformer.html#更多的数据集",
    "href": "transformer.html#更多的数据集",
    "title": "Transformer101",
    "section": "更多的数据集",
    "text": "更多的数据集\n\nxsum: 总结概述训练集\n\nd = load_dataset('xsum', split='train')\n\n\nd[1:2]\n\n{'document': ['A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"'],\n 'summary': ['Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.'],\n 'id': ['40143035']}\n\n\n\n\nAWS 评论数据集\n\nd = load_dataset('amazon_us_reviews', 'Video_v1_00')"
  },
  {
    "objectID": "transformer.html#transformer的训练流程",
    "href": "transformer.html#transformer的训练流程",
    "title": "Transformer101",
    "section": "Transformer的训练流程",
    "text": "Transformer的训练流程\n1.使用动态padding的方法创建处理数据集\n\ndata=batch_dataset(raw_datasets,padding_dynamic)\n\n2.定义好模型\n\nmodel=AutoModelForSequenceClassification.from_pretrained('prajjwal1/bert-medium')\n\n3.metric指标，函数签名要求返回一个dict\n\ndef compute_metrics(data):\n    logits,labels=data\n    \n    pred_labels=np.argmax(logits,-1)\n    acc=(pred_labels==labels).mean()\n    \n    return {\"acc\":acc}\n\ndef compute_metrics_mrpc(eval_preds):\n    metric = evaluate.load(\"glue\", \"mrpc\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    #这是evaluate返回函数的固定方法compute,根据预测与标注，返回dict（包含所有指标）\n    return metric.compute(predictions=predictions, references=labels)\n\n4.设置TrainingArguments,第一个参数表示要保存模型的路径\n\nargs=TrainingArguments('output',evaluation_strategy='epoch',)\ntrainer=Trainer(model,args,\n                train_dataset=data['train'],\n                eval_dataset=data['validation'],\n                data_collator=DataCollatorWithPadding(tokenizer),\n                tokenizer=tokenizer,\n                compute_metrics=compute_metrics\n               )\n\n\ntrainer.train()\n\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n    \n      \n      \n      [1377/1377 01:15, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAcc\n\n\n\n\n1\nNo log\n0.399317\n0.825980\n\n\n2\n0.509500\n0.513949\n0.833333\n\n\n3\n0.290800\n0.708980\n0.850490\n\n\n\n\n\n\nTrainOutput(global_step=1377, training_loss=0.3337492572283, metrics={'train_runtime': 78.7102, 'train_samples_per_second': 139.804, 'train_steps_per_second': 17.495, 'total_flos': 120856103619216.0, 'train_loss': 0.3337492572283, 'epoch': 3.0})\n\n\n5.预测测试数据集\n\n### test dataset\ntest_predict=trainer.predict(data['test'])\nprint(f'predict shape:{test_predict.predictions.shape}')\nprint(f'label shape:{test_predict.label_ids.shape}')\ntest_predict.metrics\n\n***** Running Prediction *****\n  Num examples = 1725\n  Batch size = 8\n\n\n\n\n\npredict shape:(1725, 2)\nlabel shape:(1725,)\n\n\n{'test_loss': 0.6360893845558167,\n 'test_acc': 0.6684057971014493,\n 'test_runtime': 20.242,\n 'test_samples_per_second': 85.219,\n 'test_steps_per_second': 10.671}"
  },
  {
    "objectID": "transformer.html#transformer的模型tokenizerconfig都下载的本地的哪里呢",
    "href": "transformer.html#transformer的模型tokenizerconfig都下载的本地的哪里呢",
    "title": "Transformer101",
    "section": "transformer的模型，tokenizer，config都下载的本地的哪里呢？",
    "text": "transformer的模型，tokenizer，config都下载的本地的哪里呢？\n\n! ls ~/.cache/huggingface/hub\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nmodels--bert-base-cased        tmpimfqc44k\nmodels--prajjwal1--bert-medium version.txt"
  },
  {
    "objectID": "transformer.html#load_dataset的参数含义",
    "href": "transformer.html#load_dataset的参数含义",
    "title": "Transformer101",
    "section": "load_dataset的参数含义？",
    "text": "load_dataset的参数含义？\n\n# split表示加载训练 还是 测试数据集\nload_dataset(dset_name,subset_name, split='train')\n\n#补充 * 补充babi数据集 * transformer与 pytorch模型训练"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "deepthinking",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "deepthinking",
    "section": "Install",
    "text": "Install\npip install deepthinking"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "deepthinking",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "diffuse_model.html",
    "href": "diffuse_model.html",
    "title": "Stable Diffusion",
    "section": "",
    "text": "“Stable Diffusion” 是一种文本到图像的潜在扩散模型，由 CompVis、Stability AI 和 LAION 的研究人员和工程师创建。它是在 LAION-5B 数据库的一个子集上训练的，图像尺寸为 512x512 像素。\n\n\n常用的模型结构\n\nCompVis/stable-diffusion-v1-4\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1-base\nstabilityai/stable-diffusion-2-1\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport gc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch_device='cpu'\ndef release_cache():\n  gc.collect()\n  torch.cuda.empty_cache()\n\n#把图片排成 rows,cols的网格中，先排cols,后排rows\n#其中len(imgs)=cols x rows\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\n#模型保存在： ~/.cache/huggingface/hub\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", \n#                                                revision=\"fp16\", \n                                               torch_dtype=torch.float32,mirror='bfsu')#tuna\npipe.to(torch_device)\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\n\ntorch.manual_seed(1)\n\nprompt = \"a photograph of an astronaut riding a horse\"\nimage=pipe(prompt).images[0] #每个提示产生一张图片\nimage.save('astronaut.png')\nimage.resize((128,128)) #PIL.Image，size=512,512\n\n\n\n\n\n\n\n\n\n每次生成图片迭代的次数，次数越多，效果自然约好！\n\ngens=[]\nfor s in [5,15,30,50]:\n  torch.manual_seed(1)\n  image=pipe(prompt,num_inference_steps=s).images[0] #默认inference_steps=50\n  gens.append(image)\nimage_grid(gens, 1, 4).resize((1024,256))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrelease_cache()\n\n\n\n\n\n多大表示【生成的图片】越来越匹配提示词\n越小表示【生成的图片】是一张图片\n默认是7？\n\n\nnum_rows,num_cols = 4,4\nprompts = [prompt] * num_cols #一次生成4张图\n\ngens=[]\nfor g in [1.1,3,7,14]:\n  torch.manual_seed(1)\n  imgs=pipe(prompts, guidance_scale=g).images\n  gens.extend(imgs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_grid(gens,num_rows,num_cols).resize((1024,1024))\n\n\n\n\n\n\n\n\nimage = pipe(prompt, height=512, width=768).images[0]\nimage.resize((768//2,252))\n\n\n\n\n\n\n\n\n\n\n否定的提示词,指示生成图片去掉【提示否定】的特征\n\n#维米尔风格的拉布拉多猎犬\nprompt = \"Labrador in the style of Vermeer\" \n# prompt = \"gril  in the style of Vermeer\"\ntorch.manual_seed(1000)\norigin_img=pipe(prompt).images[0]\n\ntorch.manual_seed(1000)\nneg_img=pipe(prompt,negative_prompt=\"blue\").images[0]\n\nimage_grid([origin_img,neg_img],1,2).resize((512,256))\n\n\n\n\n\n\n\n\n\n\n\n\n\nstrength: 越小越接近原来的init_image\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom fastdownload import FastDownload\nimport torch\nrelease_cache()\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n)\n\npipe.to(\"cuda\")\n\n\n#guide  image\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\ninit_image = Image.open(p).convert(\"RGB\")\ninit_image\n\n\n\n\n\ntorch.manual_seed(1000)\n\n# \"Oil painting of wolf howling at the moon by Van Gogh\"\nprompt = \"Wolf howling at the moon, photorealistic 4K\"\nimages = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=1, num_inference_steps=70).images\nimage_grid(images, rows=1, cols=3).resize((256*3,256))\n\n\n\n\n\n\n\n\n\n\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16) \npipe = pipe.to(\"cuda\")\n\nembeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\"\nembeds_path = FastDownload().download(embeds_url)\nembeds_dict = torch.load(str(embeds_path), map_location=\"cpu\")\nassert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\nnew_token, embeds = next(iter(embeds_dict.items()))\nembeds = embeds.to(text_encoder.dtype)\nassert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\nnew_token_id = tokenizer.convert_tokens_to_ids(new_token)\ntext_encoder.get_input_embeddings().weight.data[new_token_id] = embeds\n\ntorch.manual_seed(1000)\nimage = pipe(\"Labrador  in the style of &lt;watercolor-portrait&gt;\").images[0]\nimage\n\n\n\n\n\n\n\n\ninit_image_van=images[2].copy()\n\n\nfrom transformers.models.ibert.modeling_ibert import gelu\n# 梵高的一幅油画，画的是狼对着月亮嚎叫\nprompt =\"Oil painting of wolf howling at the moon by Van Gogh\"\ngen=[]\nfor x in [(0.8,50),(0.8,70),(1,50),(1,70)]:\n  images = pipe(prompt=prompt, \n                num_images_per_prompt=3, \n                image=init_image_van, \n                strength=x[0], \n                num_inference_steps=x[1]).images\n  gen.extend(images)\n\n\nimage_grid(gen, 4, 3).resize((768,1024))"
  },
  {
    "objectID": "diffuse_model.html#stablediffusionpipeline",
    "href": "diffuse_model.html#stablediffusionpipeline",
    "title": "Stable Diffusion",
    "section": "",
    "text": "常用的模型结构\n\nCompVis/stable-diffusion-v1-4\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1-base\nstabilityai/stable-diffusion-2-1\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport gc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch_device='cpu'\ndef release_cache():\n  gc.collect()\n  torch.cuda.empty_cache()\n\n#把图片排成 rows,cols的网格中，先排cols,后排rows\n#其中len(imgs)=cols x rows\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\n#模型保存在： ~/.cache/huggingface/hub\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", \n#                                                revision=\"fp16\", \n                                               torch_dtype=torch.float32,mirror='bfsu')#tuna\npipe.to(torch_device)\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\n\ntorch.manual_seed(1)\n\nprompt = \"a photograph of an astronaut riding a horse\"\nimage=pipe(prompt).images[0] #每个提示产生一张图片\nimage.save('astronaut.png')\nimage.resize((128,128)) #PIL.Image，size=512,512\n\n\n\n\n\n\n\n\n\n每次生成图片迭代的次数，次数越多，效果自然约好！\n\ngens=[]\nfor s in [5,15,30,50]:\n  torch.manual_seed(1)\n  image=pipe(prompt,num_inference_steps=s).images[0] #默认inference_steps=50\n  gens.append(image)\nimage_grid(gens, 1, 4).resize((1024,256))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrelease_cache()\n\n\n\n\n\n多大表示【生成的图片】越来越匹配提示词\n越小表示【生成的图片】是一张图片\n默认是7？\n\n\nnum_rows,num_cols = 4,4\nprompts = [prompt] * num_cols #一次生成4张图\n\ngens=[]\nfor g in [1.1,3,7,14]:\n  torch.manual_seed(1)\n  imgs=pipe(prompts, guidance_scale=g).images\n  gens.extend(imgs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_grid(gens,num_rows,num_cols).resize((1024,1024))\n\n\n\n\n\n\n\n\nimage = pipe(prompt, height=512, width=768).images[0]\nimage.resize((768//2,252))\n\n\n\n\n\n\n\n\n\n\n否定的提示词,指示生成图片去掉【提示否定】的特征\n\n#维米尔风格的拉布拉多猎犬\nprompt = \"Labrador in the style of Vermeer\" \n# prompt = \"gril  in the style of Vermeer\"\ntorch.manual_seed(1000)\norigin_img=pipe(prompt).images[0]\n\ntorch.manual_seed(1000)\nneg_img=pipe(prompt,negative_prompt=\"blue\").images[0]\n\nimage_grid([origin_img,neg_img],1,2).resize((512,256))\n\n\n\n\n\n\n\n\n\n\n\n\n\nstrength: 越小越接近原来的init_image\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom fastdownload import FastDownload\nimport torch\nrelease_cache()\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n)\n\npipe.to(\"cuda\")\n\n\n#guide  image\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\ninit_image = Image.open(p).convert(\"RGB\")\ninit_image\n\n\n\n\n\ntorch.manual_seed(1000)\n\n# \"Oil painting of wolf howling at the moon by Van Gogh\"\nprompt = \"Wolf howling at the moon, photorealistic 4K\"\nimages = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=1, num_inference_steps=70).images\nimage_grid(images, rows=1, cols=3).resize((256*3,256))\n\n\n\n\n\n\n\n\n\n\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16) \npipe = pipe.to(\"cuda\")\n\nembeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\"\nembeds_path = FastDownload().download(embeds_url)\nembeds_dict = torch.load(str(embeds_path), map_location=\"cpu\")\nassert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\nnew_token, embeds = next(iter(embeds_dict.items()))\nembeds = embeds.to(text_encoder.dtype)\nassert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n\ntext_encoder.resize_token_embeddings(len(tokenizer))\nnew_token_id = tokenizer.convert_tokens_to_ids(new_token)\ntext_encoder.get_input_embeddings().weight.data[new_token_id] = embeds\n\ntorch.manual_seed(1000)\nimage = pipe(\"Labrador  in the style of &lt;watercolor-portrait&gt;\").images[0]\nimage\n\n\n\n\n\n\n\n\ninit_image_van=images[2].copy()\n\n\nfrom transformers.models.ibert.modeling_ibert import gelu\n# 梵高的一幅油画，画的是狼对着月亮嚎叫\nprompt =\"Oil painting of wolf howling at the moon by Van Gogh\"\ngen=[]\nfor x in [(0.8,50),(0.8,70),(1,50),(1,70)]:\n  images = pipe(prompt=prompt, \n                num_images_per_prompt=3, \n                image=init_image_van, \n                strength=x[0], \n                num_inference_steps=x[1]).images\n  gen.extend(images)\n\n\nimage_grid(gen, 4, 3).resize((768,1024))"
  },
  {
    "objectID": "diffuse_model.html#stable-diffusion-推理阶段",
    "href": "diffuse_model.html#stable-diffusion-推理阶段",
    "title": "Stable Diffusion",
    "section": "Stable Diffusion 推理阶段",
    "text": "Stable Diffusion 推理阶段\n\n\n\n\n稳定扩散模型将潜在种子和文本提示作为输入。然后，使用潜在种子生成大小为 64×64 的随机潜在图像表示\n文本提示通过 CLIP 的文本编码器转换为大小为 77×768 的文本嵌入。\nU-Net 在被条件化的文本嵌入的作用下，迭代地去噪随机潜在图像表示。\n\nU-Net 的输出是噪声残差，通过调度算法(Schedular)计算去噪潜在图像表示。可以使用许多不同的调度算法进行此计算，每个算法都有其优缺点。对于 Stable Diffusion，我们建议使用以下之一：\n\nPNDM scheduler (used by default).\nK-LMS scheduler.\nHeun Discrete scheduler.\nDPM Solver Multistep scheduler. 可以较少推理步数到25"
  },
  {
    "objectID": "diffuse_model.html#vae",
    "href": "diffuse_model.html#vae",
    "title": "Stable Diffusion",
    "section": "VAE",
    "text": "VAE\nvae的目的是把图片转换成对应的latent，然后输入到unet进行生成操作，对latent的操作相对于raw image大幅度减少运算量。\n\nfrom diffusers import AutoencoderKL,UNet2DConditionModel,LMSDiscreteScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport torch,numpy as np \nfrom torchvision import transforms as tfms\nfrom  PIL import Image\n\n\nwith torch.no_grad():\n    vae=AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\",mirror='bfsu').to(torch_device)\n\nVAE encoder的输入,decoder的输出范围是在[-1,1]之间，图片的取值范围是[0,1]，所以不要忘记输入输出的线性转换.\n\ndef tfms2Img(r):\n    r=(0.5*r+0.5)\n    return r.float().clamp(0,1)\ndef tfms2Latent(r):\n    return 2*r-1\n\nwith torch.no_grad():\n    I=torch.rand(1,3,512,512).to(vae.device)\n    I=tfms2Latent(I)\n    # 编码\n    latents=vae.encode(I).latent_dist.sample()\n    # 解码\n    I_pred=vae.decode(latents).sample\n  \n    I_pred=tfms2Img(I_pred)\n    print(I_pred.min(),I_pred.max())\n\ntensor(0.) tensor(1.)\n\n\n\ninput_image = Image.open('yangyang.jpg').rotate(0).resize((512,512))\ninput_image\n\n\n\n\n\n@torch.no_grad()\ndef pil2Latents(input_image:Image)-&gt; torch.FloatTensor:\n    '''\n    把图片转换成vae的输入\n    返回：size=[1,4,64,64]\n    '''\n    ts=tfms.ToTensor()(input_image).unsqueeze(0)\n    ts=tfms2Latent(ts).to(vae.device)\n    return 0.18215*vae.encode(ts).latent_dist.sample()\n@torch.no_grad()\ndef latents2Pil(latents:torch.FloatTensor) -&gt;Image:\n    '''\n    把隐变量还原成PIL.Image\n    latents: FloatTensor,size=[1,4,64,64]\n    '''\n    decode_img=vae.decode(latents/0.18215).sample.detach().cpu()\n    decode_img=decode_img.permute(0,2,3,1).squeeze()\n    decode_img=tfms2Img(decode_img)\n\n    arr_img=decode_img.numpy()*255\n    arr_img=arr_img.astype('uint8')\n    return Image.fromarray(arr_img)\n\n\nwith torch.no_grad():\n    z=pil2Latents(input_image)\n    input_image_decoded=latents2Pil(z)\ninput_image_decoded.resize((256,256))\n\n\n\n\n下面代码用于查看下latent，可以看出1,2通道的latent表示的是图片的内容，3，4通道的latent表示的是图片的细节\n\nnp_latent=z.cpu().numpy()[0]\n_,axes=plt.subplots(1,4)\n\nfor i in range(4):\n    axes[i].imshow(np_latent[i],cmap='gray')\n    axes[i].axis('off')"
  },
  {
    "objectID": "diffuse_model.html#cliptextmodel",
    "href": "diffuse_model.html#cliptextmodel",
    "title": "Stable Diffusion",
    "section": "CLIPTextModel",
    "text": "CLIPTextModel\nCLIP模型根据提示词prompt，驱动unet生成我们描述的图片。huggingface的stable-diffusion使用的是openai/clip-vit-base-patch32模型，此模型并没有参与过sd的训练。\n\n#subfolder参数 可以理解成子类\ntokenizer    = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\",mirror='tuna')\ntext_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\",mirror='tuna')\n\n使用text_encoder,直接对prompt进行编码，生成对应的embedding。\n\nprompt='Drake as a pokemon, small, monster, manga'\ninput_text=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,truncation=True,return_tensors='pt')\nprint(\"inp.size:\",input_text.input_ids.shape)\n\n\nwith torch.no_grad():\n    #只输入input_ids，不输入attention_mask,为什么呢？\n    out_encoder=text_encoder(input_text.input_ids)\n    print(\"embbeding size:\",out_encoder.last_hidden_state.shape)\n\ninp.size: torch.Size([1, 77])\nembbeding size: torch.Size([1, 77, 768])\n\n\n注意 从上面的例子我们可以看出,clip-vit其实是个解码器的结构，\\(h_t\\)的输出只依赖与\\(x_0,x_1,x_t\\)\n下面可以看到，索引与对应subword的关系\n\nfor idx in input_text.input_ids[0][:12]:\n    print(f\"{idx}  {tokenizer.decoder.get(int(idx))}\")\n\n49406  &lt;|startoftext|&gt;\n8958  drake&lt;/w&gt;\n601  as&lt;/w&gt;\n320  a&lt;/w&gt;\n9528  pokemon&lt;/w&gt;\n267  ,&lt;/w&gt;\n2442  small&lt;/w&gt;\n267  ,&lt;/w&gt;\n6060  monster&lt;/w&gt;\n267  ,&lt;/w&gt;\n11873  manga&lt;/w&gt;\n49407  &lt;|endoftext|&gt;\n\n\n\n模型拆解，embdding,encoder,final_layers\n\nmodel_embeddings=text_encoder.text_model.embeddings\nmodel_encoder   =text_encoder.text_model.encoder\nmodel_finallayer=text_encoder.text_model.final_layer_norm\n\n分两步进行输出，第一步输出token_embdding,第二步输出transform_final_embbeding\n\nout_emb=model_embeddings(input_text.input_ids)\n\ndef get_output_embds(out_emb):\n    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(1, tokenizer.model_max_length, dtype=out_emb.dtype)\n    print(causal_attention_mask[0,0,:4,:4])\n    out_encoder=model_encoder(inputs_embeds=out_emb,\n                              causal_attention_mask=causal_attention_mask,\n                              output_hidden_states=True)\n    out_final=model_finallayer(out_encoder.last_hidden_state)\n\n    return out_final\n\nout_emb_final=get_output_embds(out_emb)\n\ntensor([[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n        [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n\n\n注意： 输入给transformer encoder的是一个causal_attention_mask，这个mask说明yt=encoder(xt_1,xt_2…)\n最后比较我的输出与pipeline.text_encoder的输出\n\n(out_emb_final-out_encoder.last_hidden_state).abs().max()\n\ntensor(0., grad_fn=&lt;MaxBackward1&gt;)\n\n\n\n\ntoken_embedding与position_embedding\n传给trasnformer的输入是一个3-rank tensor,由(B,T,D)三个维度组成。 * token_embedding 是把词索引转换成词向量的函数。F: (B,T) -&gt;(B,T,D)。 * position_embedding 是把位置索引 转换成 向量的函数。F:(T,)-&gt; (T,D)。\n\n#验证输出=token_embedding+position_embedding\nguide_ids=tokenizer(\"A clear gril sitting\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\nep=text_encoder.text_model.embeddings(guide_ids)\n\ne1=text_encoder.text_model.embeddings.token_embedding(guide_ids) \np1=text_encoder.text_model.embeddings.position_embedding(torch.arange(77))\n(ep-e1-p1).abs().max()\n\ntensor(5.9605e-08, grad_fn=&lt;MaxBackward1&gt;)"
  },
  {
    "objectID": "diffuse_model.html#unet",
    "href": "diffuse_model.html#unet",
    "title": "Stable Diffusion",
    "section": "UNet",
    "text": "UNet\nUnet把 图片的latent 与 prompt embbeding 作为输入，输出 latent中的 noise.\n\nunet=UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder='unet',mirror='tuna')\nunet.to(torch_device)\n\n# unet=UNet2DConditionModel.from_config(\"CompVis/stable-diffusion-v1-4\",subfolder='unet',mirror='tuna').to(torch_device)\n\n准备unet的输入(latent,text_embbeding),然后简单的验证一下即可\n\nu_inp_latent=torch.randn(1,4,64,64).to(torch_device)\nu_inp_embbeding=torch.randn(1,77,768).to(torch_device)\n\nnoise_pred = unet1(u_inp_latent, torch.tensor(1), encoder_hidden_states=u_inp_embbeding).sample\nprint(noise_pred.shape)\n\ntorch.Size([1, 4, 64, 64])"
  },
  {
    "objectID": "diffuse_model.html#scheduler",
    "href": "diffuse_model.html#scheduler",
    "title": "Stable Diffusion",
    "section": "Scheduler",
    "text": "Scheduler\n目前我对schedular的初始化参数还不理解，需要进一步深入学习,目前我了解的schedular的作用主要为：\n\ntraining的时候，对图片的隐变量加入noise\ninference的时候 缩小 latent输入。\ninference的时候 每一次迭代，执行latent=latent-sigma*noise操作。\n每个timestamp对应的输入noise level是不同的，t越小，noise越小\n\n我的困惑是： - 不同timesteps下的 sigma是如何确定？ - DDPM,DDIM和 下面所用的LMSDiscreteScheduler有什么关系？ - 不同Schedular的区别是什么，具体参数的含义？\n\nnum_train_timesteps=30\nschedular=LMSDiscreteScheduler(beta_schedule='scaled_linear',beta_start=0.00085, beta_end=0.012, )\nschedular.set_timesteps(num_train_timesteps)\n\n\nplt.plot(schedular.timesteps,schedular.sigmas[1:])\nplt.xlabel('steps')\nplt.ylabel('$\\sigma$')\n\nText(0, 0.5, '$\\\\sigma$')\n\n\n\n\n\nschedular在训练的时候是对一张图片add noise，推理的时候决定如何denoise,我们通过以下代码，来看看不同t下对应的图片与noise关系\n\nwith torch.no_grad():\n    z=pil2Latents(input_image)\n    noise=torch.rand_like(z).to(torch_device)\n    gens=[]\n    for s in [29,25,20,15]:\n        z_noise=schedular.add_noise(z,noise,torch.tensor([schedular.timesteps[s]]))\n        input_image_decoded=latents2Pil(z_noise)\n        gens.append(input_image_decoded)\n\n\nimage_grid(gens,1,4).resize((512*4,512))"
  },
  {
    "objectID": "diffuse_model.html#完整的推理流程",
    "href": "diffuse_model.html#完整的推理流程",
    "title": "Stable Diffusion",
    "section": "完整的推理流程",
    "text": "完整的推理流程\n我们准备好我们需要的模型，vae,clip_tokener,clip_encoder,unet,schedular\n\nmodelName=\"CompVis/stable-diffusion-v1-4\"\nmirror='bfsu'\n\n\nvae=AutoencoderKL.from_pretrained(modelName,subfolder='vae',mirror=mirror).to(torch_device)\ntokenizer=CLIPTokenizer.from_pretrained(modelName,subfolder=\"tokenizer\",mirror=mirror)\ntext_encoder=CLIPTextModel.from_pretrained(modelName,subfolder=\"text_encoder\",mirror=mirror).to(torch_device)\nunet=UNet2DConditionModel.from_pretrained(modelName,subfolder='unet',mirror=mirror).to(torch_device)\nschedular=LMSDiscreteScheduler(beta_schedule='scaled_linear',beta_start=0.00085, beta_end=0.012 )\n\n我们也可以直接从pipe中获得，这里的模型我用了单精度的。\n\npipe = StableDiffusionPipeline.from_pretrained(modelName, \n                                               revision=\"fp16\", \n                                               mirror=mirror,\n                                               torch_dtype=torch.float16)\npipe.to(torch_device)\n\nvae=pipe.vae\ntokenizer=pipe.tokenizer\ntext_encoder=pipe.text_encoder\nunet=pipe.unet\nschedular=pipe.scheduler\n\n0.setup\n\nprompt=\"A clear boy,catton style\"\nguide_factor=7.5\nnum_of_inference=50\nschedular.set_timesteps(num_of_inference)\n\n1.使用clip_encoder，生成【无prompt】和【prompt】的embbeding\n注意：对于class free模型，是把 【无prompt】和【prompt】生成的noise 根据guide_factor进行组合后生成最终的final_noise，再去执行denoise操作。\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n    token_emb=text_encoder(ids).last_hidden_state\n\n2.循环推理逻辑，输入model之前先scale 输入，生成pred_noise执行schedular.step,消除噪声。\n\ntorch.manual_seed(1000)\nimg_latent=torch.randn(1,4,64,64).to(torch_device)\nimg_latent*=schedular.init_noise_sigma\n\nwith torch.no_grad():\n    for t in tqdm(schedular.timesteps):\n        inp=torch.concat([img_latent,img_latent])\n        inp=schedular.scale_model_input(inp,t)\n        \n        noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n        \n        pred_noise=noise[0]+guide_factor*(noise[1]-noise[0])\n        \n        img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n\n\n\n\n3.最后解码还原图片\n\nfinal_latent=img_latent.detach()\nlatents2Pil(final_latent)\n\n\n\n\n\nA.guidance parameter update\n存在一下4种调整\n\ndef norm_none(u,g,t):\n    return u+g*(t-u)\ndef norm_diff(u,g,t):\n    'arunoda suggest'\n    v=(t-u)/(t-u).norm()\n    return u+g*u.norm()*v\ndef norm_whole(u,g,t): \n    'jeremy suggest'\n    v=norm_none(u,g,t)\n    return u.norm()*(v/v.norm())\ndef norm_diff_whole(u,g,t):\n    'arunoda final'\n    v=(t-u)/(t-u).norm()\n    p=u+g*u.norm()*v\n    return p/p.norm()*u.norm()\n\n\ndef compare_guide(prompt,norm_method,guide_factor):\n    num_of_inference=50\n    schedular.set_timesteps(num_of_inference)\n\n\n    no_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n    guide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\n    with torch.no_grad():\n        ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n        token_emb=text_encoder(ids).last_hidden_state\n\n    torch.manual_seed(442)\n    img_latent=torch.randn(1,4,64,64).to(torch_device)\n    img_latent*=schedular.init_noise_sigma\n\n    with torch.no_grad():\n        for t in tqdm(schedular.timesteps):\n            inp=torch.concat([img_latent,img_latent])\n            inp=schedular.scale_model_input(inp,t)\n\n            noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n\n            pred_noise=norm_method(noise[0],guide_factor,noise[1])\n\n            img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n    final_latent=img_latent.detach()\n    return latents2Pil(final_latent)\n\n比较下效果\n\nprompt=\"a photograph of an astronaut riding a horse\"\ngen=[]\nfs=[norm_none,norm_whole,norm_diff,norm_diff_whole]\ngs=[7.5,7.5,0.15,0.15]\nfor m,g in zip(fs,gs):\n    gen.append(compare_guide(prompt,m,g))\nimage_grid(gen,2,2)\n\n\n\n\n\n\n动态调整guide_scale\n我们使用cosfit的方式调整guild_scalar的消息，具体的公式为：\n\\[g(t)=\\frac{A}{2}[1+cos(\\frac{\\pi t}{S})]\\]\n\nsteps,scalar=50,7.5\n\nxx=np.linspace(0,steps-1,steps)\nyy_constant=[scalar]*steps\nyy_adjust=(scalar/2)*(1+np.cos(np.pi/steps * xx))\n\nplt.plot(xx,yy_constant,label='const')\nplt.plot(xx,yy_adjust,label='cos')\nplt.xlabel('steps')\nplt.ylabel('guild factor')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x13c812230&gt;\n\n\n\n\n\n\ndef get_guide_factor(t):\n    return (guide_factor/2)*(1+np.cos(np.pi/num_of_inference * t))\n\n\nprompt=\"A clear boy,catton style\"\nguide_factor=7.5\nnum_of_inference=50\nschedular.set_timesteps(num_of_inference)\n\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n    token_emb=text_encoder(ids).last_hidden_state\n    \ntorch.manual_seed(1000)\nimg_latent=torch.randn(1,4,64,64).to(torch_device)\nimg_latent*=schedular.init_noise_sigma\n\nwith torch.no_grad():\n    for k,t in tqdm(enumerate(schedular.timesteps)):\n        inp=torch.concat([img_latent,img_latent])\n        inp=schedular.scale_model_input(inp,t)\n        \n        noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n        \n        pred_noise=noise[0]+get_guide_factor(k)*(noise[1]-noise[0])\n        \n        img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n        break\nlatents2Pil(img_latent)"
  },
  {
    "objectID": "diffuse_model.html#基于embedding的微调应用",
    "href": "diffuse_model.html#基于embedding的微调应用",
    "title": "Stable Diffusion",
    "section": "基于embedding的微调应用",
    "text": "基于embedding的微调应用\n\nImage2Image\n先定义一张初始化图片\n\nwith torch.no_grad():\n    init_image = Image.open('yangyang.jpg').rotate(0).resize((512,512))\n    init_latents=pil2Latents(init_image)\n    im=latents2Pil(init_latents)\nim.resize((128,128))\n\n\n\n\n与【完整推理流程一样】，把提示词向量化，生成对应的token_emb\n\nprompt=\"A clear boy,catton style\"\n# torch.manual_seed(32)\nguide_factor=8\nnum_of_inference=50\nstart_step=27\n\nschedular.set_timesteps(num_of_inference)\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n    token_emb=text_encoder(ids).last_hidden_state\n\n对初始化图片加入noise后，进行推理\n\ntorch.manual_seed(1000)\nimg_latent=torch.randn_like(init_latents)\nimg_latent=schedular.add_noise(init_latents,img_latent,schedular.timesteps[start_step:start_step+1])\n# img_latent*=schedular.init_noise_sigma\nimg_latent.to(torch_device)\n\n\n#bug1 img_latent=init_latents,这样生成的图片内容就是init_latents,不会受提示词控制\n#bug2 schedular.add_noise没返回值,随机的noise，然后从start_step开始推理，引入的noise过小\n#bug3 schedular.add_noise(img_latent,init_latents),把初始化图片当初noise\n#bug4 img_latent*=schedular.init_noise_sigma 或者 img_latent*=schedular.sigma[t],引入过多的Noise\n\nwith torch.no_grad():\n    for s in tqdm( range(len(schedular.timesteps)) ):\n        if s&gt;=start_step:     \n            t=schedular.timesteps[s]\n            inp=torch.concat([img_latent,img_latent])\n            inp=schedular.scale_model_input(inp,t)\n\n            noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n\n            # pred_noise=noise[0]+guide_factor*(noise[1]-noise[0])\n            pred_noise=norm_whole(noise[0],guide_factor,noise[1])\n            img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\nlatents2Pil(img_latent)\n\n\n\n\n注意以下事项 - 如果最终生成的图片花屏,说明加入的noise过大 - 如果最终生成单色的图片，那是因为schedular对图片的scale有问题。我犯过一个Bug是隐变量是随机的，从start_step=25步开始推理，这样导致schedular的noise等级比实际latent等级低很多，最终导致生成了单色图片 - latent的初始值应该是 【初始化图片】+[noise]。我犯过bug1是没有加入noise，这样prompt的指导作用完全消失。bug2,noise与【初始化图片】位置颠倒\n\n\nNegtive Prompt\n否定的提示词实现，需要对text_encoder中embbeding的输出的【Prompt】与【negPrompt】的词向量进行操作，再输入到之后的网络模型。\n\nmodelName=\"CompVis/stable-diffusion-v1-4\"\nmirror=None\npipe = StableDiffusionPipeline.from_pretrained(modelName, \n                                               revision=\"fp16\", \n                                               mirror=mirror,\n                                               torch_dtype=torch.float16)\npipe.to(torch_device)\nvae=pipe.vae\ntokenizer=pipe.tokenizer\ntext_encoder=pipe.text_encoder\nunet=pipe.unet\nschedular=pipe.scheduler\n\nKeyword arguments {'mirror': None} are not expected by StableDiffusionPipeline and will be ignored.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n\n\n\ndef generate_baseon_prompt_emb(token_emb:torch.FloatTensor)-&gt;Image:\n    '''\n        :param token_emb: 句子经过clip_encoder后的向量表示,(2,T,D) \n        :return:  denoise 后的图片\n    '''\n    img_latent=torch.randn(1,4,64,64,dtype=token_emb.dtype).to(token_emb.device)\n    img_latent*=schedular.init_noise_sigma\n\n    with torch.no_grad():\n        for t in tqdm(schedular.timesteps):\n            inp=torch.concat([img_latent,img_latent])\n            inp=schedular.scale_model_input(inp,t)\n\n            noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n\n            pred_noise=noise[0]+guide_factor*(noise[1]-noise[0])\n            # pred_noise=noise[0]+guide_factor*(noise[1]-noise[0])/torch.norm(noise[1]-noise[0])*torch.norm(noise[0])\n            img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n    return latents2Pil(img_latent)\n\ndef get_output_embds(out_emb:torch.FloatTensor)-&gt;torch.FloatTensor:\n    '''\n        :param out_emb: 句子的词向量表示,(B,T,D) \n        :return:  out_emb经过text_encoder的输出\n    '''\n    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(2, tokenizer.model_max_length, dtype=out_emb.dtype)\n    out_encoder=text_encoder.text_model.encoder(\n                                inputs_embeds=out_emb,\n                                causal_attention_mask=causal_attention_mask.to(out_emb.device),\n                                output_hidden_states=True)\n    out_final=text_encoder.text_model.final_layer_norm(out_encoder.last_hidden_state)\n\n    return out_final\n\n尝试把两个词向量相减,如果直接想减得到的是垃圾，我这里把neg prompt的词向量缩小后再想减。\n\nprompt = \"A busy street in Paris on a summer day\" \n# prompt=\"Alaskan Malamute\"\nneg_prompt=\"tree\"\n\nguide_factor=7.5\nnum_of_inference=50\n\n\nschedular.set_timesteps(num_of_inference)\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_neg_ids=tokenizer(neg_prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\nweight=np.linspace(0,0.1,4)\nall_embs=[]\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids,guide_neg_ids]).to(torch_device)\n    tmp_emb=text_encoder.text_model.embeddings(ids)\n    # tmp_emb[0]-&gt;u, tmp_emb[1]-&gt;t,tmp_emb[2]-&gt;n\n    uncondition_emb=tmp_emb[0]\n    e1,e2=tmp_emb[1],tmp_emb[2]\n    for w in weight:\n        t=e1-w*e2\n        token_emb_neg=get_output_embds(torch.stack([uncondition_emb,t]))\n        all_embs.append(token_emb_neg)\n\n显示结果\n\ngen=[]\nfor token_emb in all_embs:\n    torch.manual_seed(1000)\n    im=generate_baseon_prompt_emb(token_emb)\n    gen.append(im)\n\nimage_grid(gen, 2, 4).resize((2*512,512))\n\n\n\n\n方法2, 输出的noise，使用类似guide的方式进行想减,我们定义以下5种生成最终pred_noise的方式，比较最终的结果\n\ndef neg_pred_noise_1(noise):\n    u,ta,n=noise.chunk(3)\n    return n+guide_factor*(ta-n)\ndef neg_pred_noise_2(noise):\n    u,ta,n=noise.chunk(3)\n    return u+guide_factor*(ta-n)\ndef neg_pred_noise_3(noise):\n    u,ta,n=noise.chunk(3)\n    return u+guide_factor*(ta-n)+guide_factor*(ta-u)\ndef neg_pred_noise_4(noise):\n    u,ta,n=noise.chunk(3)\n    return  u+guide_factor/2*(ta-n)+guide_factor/2*(ta-u)\ndef neg_pred_noise_5(noise):\n    u,ta,n=noise.chunk(3)\n    return u+guide_factor*(ta-n-u)\n\n\ndef sd_inference(img_latent,token_emb,get_predict_noise)-&gt;Image:\n    '''\n        根据输入的隐变量(img_latent)与提示词(token_emb),返回最终解析的图片.我们加入参数\n        get_predict_noise，把如何生成predict_noise的任务交给了调用者来觉得。\n        \n        :param img_latent:(4,64,64)\n        :param  token_emb:(B,T,D)\n        :get_predict_noise: 函数签名 f(noise) -&gt; pred_noise,pred_noise是(4,64,64)的tensor\n    '''\n    with torch.no_grad():\n        for t in tqdm(schedular.timesteps):\n            inp=torch.concat([img_latent]*token_emb.shape[0])\n            inp=schedular.scale_model_input(inp,t)\n\n            noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n            pred_noise=get_predict_noise(noise)\n            \n\n            img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n            break\n    return latents2Pil(img_latent)\n\n\nprompt = \"A busy street in Paris on a summer day\" \n# prompt=\"Alaskan Malamute\"\nneg_prompt=\"tree\"\n\nguide_factor=7.5\nnum_of_inference=50\n\n\nschedular.set_timesteps(num_of_inference)\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_neg_ids=tokenizer(neg_prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids,guide_neg_ids])\n    # tmp_emb[0]-&gt;u, tmp_emb[1]-&gt;ta,tmp_emb[2]-&gt;n\n    token_emb=text_encoder(ids).last_hidden_state.to(torch_device)\n\n\n\n\ngens=[]\nfor f in [neg_pred_noise_5]:\n    torch.manual_seed(1000)\n    img_latent=torch.randn(1,4,64,64).to(torch_device)\n    img_latent*=schedular.init_noise_sigma\n    I=sd_inference(img_latent,token_emb,f)\n    gens.append(I)\n\n\n\n\n\n# image_grid(gens,2,4)\n\n\n\nMix Embeddings\n我们也可以把clip_encoder的输出向量进行融合，然后生成图片\n\nprompt1=\"a mountain, cinematic angle, studio Ghibli, cinematic lighting, detailed oil painting, hyperrealistic, 8k\"\nprompt2=\"A long and winding beach, tropical, bright, simple, by Studio Ghibli and Greg Rutkowski, artstation\"\n\n\nguide_factor=7.5\nnum_of_inference=50\n\n\nschedular.set_timesteps(num_of_inference)\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids_1=tokenizer(prompt1,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids_2=tokenizer(prompt2,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids_1,guide_ids_2]).to(torch_device)\n    tmp_emb=text_encoder(ids).last_hidden_state\nmix_factor=0.2\ntmp_emb[1]=mix_factor*tmp_emb[1]+(1-mix_factor)*tmp_emb[2]\ngenerate_baseon_prompt_emb(tmp_emb[0:2])\n\n\n\n\n\n\nTextual Inversion\nTextual Inversion是通过 embedding层，生成 某种风格所对应的一个新token的词向量,然后用这个新token的词向量作为提示词，生成图片\n1.下载预训练好的词向量\n\n!wget https://huggingface.co/sd-concepts-library/madhubani-art/resolve/main/learned_embeds.bin\n\n\npath=\"learned_embeds.bin\"\nembbed=torch.load(path,map_location=torch.device(torch_device))\nkey='&lt;madhubani-art&gt;'\n\n2.加入新的新token的词向量作为提示词的输入\n\nprompt = \"Labrador in the style of\" \n\n\nguide_factor=7.5\nnum_of_inference=50\n\n\nschedular.set_timesteps(num_of_inference)\n\nno_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\nguide_ids=tokenizer(prompt,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n\n\nwith torch.no_grad():\n    ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n    tmp_emb=text_encoder.text_model.embeddings(ids)\n    \n    \n    #获得所有49407token的索引位置\n    _,indexC=torch.where(guide_ids==49407)\n    #替换成madhubani的风格\n    tmp_emb[1][indexC[0]]=embbed[key]\n\n    token_emb=get_output_embds(tmp_emb)\ngenerate_baseon_prompt_emb(token_emb)"
  }
]