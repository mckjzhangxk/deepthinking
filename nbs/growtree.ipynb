{
 "cells": [
  {
   "cell_type": "raw",
   "id": "05db5146-9d82-4925-9f09-551d75dd0bdf",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"growthree\"\n",
    "toc: true\n",
    "toc-expand: 2\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287096d8-6646-43ce-861b-44433c878f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp growthree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6_igh6AqWTF5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "!pip install -Uq diffusers transformers fastcore\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f368e-a412-46d0-ab78-420006735e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,numpy as np,math,matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tfms\n",
    "from diffusers import StableDiffusionPipeline,AutoencoderKL,UNet2DConditionModel,LMSDiscreteScheduler\n",
    "from transformers import CLIPTokenizer,CLIPTextModel\n",
    "from PIL import Image\n",
    "import gc,os\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06128a7-2cee-4526-8f5d-058291572f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fa747-6640-4691-ba5e-03d10897bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前环境不是Colab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 'cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def release_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "#把图片排成 rows,cols的网格中，先排cols,后排rows\n",
    "#其中len(imgs)=cols x rows\n",
    "def image_grid(imgs, rows, cols):\n",
    "    w,h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "def isColab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "    if IN_COLAB:\n",
    "        print(\"当前环境是Colab\")\n",
    "    else:\n",
    "        print(\"当前环境不是Colab\")\n",
    "    return IN_COLAB\n",
    "isColab(),torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a272221-b2b5-486d-b2a2-f80b0a0980d9",
   "metadata": {},
   "source": [
    "## 超参设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5cf6f-07ba-4c65-b6ff-0b2ca487a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class HiperParameter():\n",
    "    prompts:tuple\n",
    "    fps:int=1\n",
    "    prompt_per_second:int=1\n",
    "    \n",
    "    seed=1000\n",
    "    guide_factor:int=7.5\n",
    "    num_inference:int=5\n",
    "    start_step:int=0\n",
    "    \n",
    "    output_dir='grow_output'\n",
    "tree_prompts =(\n",
    "    \"An oak tree with bare branches in the winter snowing blizzard bleak\",\n",
    "    \"A barren oak tree with no leaves and grass on the ground\",\n",
    "    \"An oak tree in the spring with bright green leaves\",\n",
    "    \"An oak tree in the summer with dark green leaves with a squirrel on the trunk\",\n",
    "    \"An oak tree in the fall with colorful leaves on the ground\",\n",
    "    \"An barren oak tree with no leaves in the fall leaves on the ground long shadows\",\n",
    "    \"An oak tree with bare branches in the winter snowing blizzard bleak\"\n",
    ")\n",
    "hparam=HiperParameter(prompts=tree_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb701e-3a4a-4c80-b907-7bdaf3cda101",
   "metadata": {},
   "source": [
    "\n",
    "\"An oak tree with bare branches in the winter snowing blizzard bleak\"：一棵冬天光秃秃的橡树，在暴风雪中，景色凄凉。\n",
    "\n",
    "\"A barren oak tree with no leaves and grass on the ground\"：一棵光秃秃的橡树，没有叶子，地上长满了草。\n",
    "\n",
    "\"An oak tree in the spring with bright green leaves\"：一棵春天长满鲜绿叶子的橡树。\n",
    "\n",
    "\"An oak tree in the summer with dark green leaves with a squirrel on the trunk\"：一棵夏天叶子深绿的橡树，树干上有一只松鼠。\n",
    "\n",
    "\"An oak tree in the fall with colorful leaves on the ground\"：一棵秋天落叶满地的橡树，树叶色彩斑斓。\n",
    "\n",
    "\"An barren oak tree with no leaves in the fall leaves on the ground long shadows\"：一棵秋天光秃秃的橡树，地上落满了树叶，形成了长长的阴影。\n",
    "\n",
    "\"An oak tree with bare branches in the winter snowing blizzard bleak\"：一棵冬天光秃秃的橡树，在暴风雪中，景色凄凉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1958bc-33fb-4d41-8782-16f8939cbd5f",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c881f06-f4f0-4dbf-a124-7f1592d73b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前环境不是Colab\n",
      "当前环境不是Colab\n"
     ]
    }
   ],
   "source": [
    "modelName=\"CompVis/stable-diffusion-v1-4\"\n",
    "mirror='bfsu' if isColab() else None\n",
    "\n",
    "if isColab():\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(modelName, \n",
    "                                                   revision=\"fp16\", \n",
    "                                                   mirror=mirror,\n",
    "                                                   torch_dtype=torch.float16)\n",
    "    pipe.to(torch_device)\n",
    "\n",
    "    vae=pipe.vae\n",
    "    tokenizer=pipe.tokenizer\n",
    "    text_encoder=pipe.text_encoder\n",
    "    unet=pipe.unet\n",
    "    schedular=pipe.scheduler\n",
    "else:\n",
    "    vae=AutoencoderKL.from_pretrained(modelName,subfolder='vae',mirror=mirror).to(torch_device)\n",
    "    tokenizer=CLIPTokenizer.from_pretrained(modelName,subfolder=\"tokenizer\",mirror=mirror)\n",
    "    text_encoder=CLIPTextModel.from_pretrained(modelName,subfolder=\"text_encoder\",mirror=mirror).to(torch_device)\n",
    "    unet=UNet2DConditionModel.from_pretrained(modelName,subfolder='unet',mirror=mirror).to(torch_device)\n",
    "    schedular=LMSDiscreteScheduler(beta_schedule='scaled_linear',beta_start=0.00085, beta_end=0.012 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f48593-1d3f-4574-84bb-6f60bedef31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def pil2Latents(input_image:Image)-> torch.FloatTensor:\n",
    "    '''\n",
    "    把图片转换成vae的输入，生成的tensor已经移torch_device上了\n",
    "    返回：size=[1,4,64,64]\n",
    "    '''\n",
    "    def tfms2Latent(r):\n",
    "        return 2*r-1\n",
    "    ts=tfms.ToTensor()(input_image).unsqueeze(0)\n",
    "    ts=tfms2Latent(ts).to(vae.device,dtype=vae.dtype)\n",
    "    return 0.18215*vae.encode(ts).latent_dist.sample()\n",
    "@torch.no_grad()\n",
    "def latents2Pil(latents:torch.FloatTensor) ->Image:\n",
    "    '''\n",
    "    把隐变量还原成PIL.Image\n",
    "    latents: FloatTensor,size=[1,4,64,64]\n",
    "    '''\n",
    "    def tfms2Img(r):\n",
    "        r=(0.5*r+0.5)\n",
    "        return r.float().clamp(0,1)\n",
    "    decode_img=vae.decode(latents/0.18215).sample.detach().cpu()\n",
    "    decode_img=decode_img.permute(0,2,3,1).squeeze()\n",
    "    decode_img=tfms2Img(decode_img)\n",
    "\n",
    "    arr_img=decode_img.numpy()*255\n",
    "    arr_img=arr_img.astype('uint8')\n",
    "    return Image.fromarray(arr_img)\n",
    "\n",
    "@torch.no_grad()\n",
    "def batchPil2Latents(imgs:list)-> torch.FloatTensor:\n",
    "    '''\n",
    "    批量把图片转换成vae的输入，生成的tensor已经移torch_device上了\n",
    "    返回：size=[B,4,64,64]\n",
    "    '''\n",
    "    def tfms2Latent(r):\n",
    "        return 2*r-1\n",
    "    f=tfms.ToTensor()\n",
    "    \n",
    "    ts=torch.stack([f(img) for img in imgs])\n",
    "    ts=tfms2Latent(ts).to(vae.device,dtype=vae.dtype)\n",
    "    return 0.18215*vae.encode(ts).latent_dist.sample()\n",
    "@torch.no_grad()\n",
    "def batchLatents2Pil(latents:torch.FloatTensor) ->Image:\n",
    "    '''\n",
    "    批量把隐变量还原成PIL.Image\n",
    "    latents: FloatTensor,size=[B,4,64,64]\n",
    "    '''\n",
    "    def tfms2Img(r):\n",
    "        r=(0.5*r+0.5)\n",
    "        return r.float().clamp(0,1)\n",
    "    decode_imgs=vae.decode(latents/0.18215).sample.detach().cpu()\n",
    "    decode_imgs=decode_imgs.permute(0,2,3,1)\n",
    "    \n",
    "    decode_imgs=tfms2Img(decode_imgs)\n",
    "\n",
    "    arr_img=decode_imgs.numpy()*255\n",
    "    arr_img=arr_img.astype('uint8')\n",
    "    return [Image.fromarray(a) for a in arr_img]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5d673-67da-46f8-af22-5d35ea1d6128",
   "metadata": {},
   "source": [
    "## 准备嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da202810-9795-41cf-a0c7-4825dacf2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O oak_tree.jpg \"https://github.com/johnrobinsn/diffusion_experiments/blob/scenario1/images/oak_tree.jpg?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf925084-788c-402f-a80f-5fcd1e1d694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_path='oak_tree.jpg'\n",
    "\n",
    "img=Image.open(tree_path)\n",
    "img_latent=pil2Latents(img)\n",
    "\n",
    "no_guide_ids=tokenizer(\"\",padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n",
    "guide_ids=tokenizer(hparam.prompts,padding='max_length',max_length=tokenizer.model_max_length,return_tensors='pt').input_ids\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    ids=torch.cat([no_guide_ids,guide_ids]).to(torch_device)\n",
    "    token_emb=text_encoder(ids).last_hidden_state\n",
    "uncondition_token_emb=token_emb[0]\n",
    "prompts_token_emb=token_emb[1:]\n",
    "\n",
    "\n",
    "def mix_embbed_and_uncond(i,j,wi,wj,V,u):\n",
    "    i=i%len(V)\n",
    "    j=j%len(V)\n",
    "\n",
    "    v=V[i]*wi+V[j]*wj\n",
    "    return torch.stack([u,v])\n",
    "mix_frame_emb=partial(mix_embbed_and_uncond,V=prompts_token_emb,u=uncondition_token_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5e2d0-aafb-4b2c-b6b2-774f3a2e16a8",
   "metadata": {},
   "source": [
    "**注意** 使用partial的函数，需要赋值的参数要在参数列表的最后面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec47bd-453f-42ec-817d-5a3e923314e8",
   "metadata": {},
   "source": [
    "## 生成每一帧图像的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9d7b6-ee95-42f2-a9af-d2464a310b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseon_prompt_emb(img_latent:torch.FloatTensor,token_emb:torch.FloatTensor,pbar=None)->(Image,torch.FloatTensor):\n",
    "    '''\n",
    "        classes free model\n",
    "        :param token_emb: img_latent 图像的隐变量,(2,4,64,64)的tensor\n",
    "        :param token_emb: 句子经过clip_encoder后的向量表示,(2,T,D) \n",
    "        :return:  denoise 后的图片\n",
    "    '''\n",
    "    if hparam.seed>=0:\n",
    "        torch.manual_seed(hparam.seed)\n",
    "    else:\n",
    "        torch.seed()\n",
    "\n",
    "    start_step=hparam.start_step\n",
    "    schedular.set_timesteps(hparam.num_inference)\n",
    "    \n",
    "    init_noise=torch.randn_like(img_latent)\n",
    "    img_latent=schedular.add_noise(img_latent,init_noise,schedular.timesteps[start_step:start_step+1])\n",
    "\n",
    "    if pbar is None:\n",
    "        pbar=tqdm(range(len(schedular.timesteps)))\n",
    "    with torch.no_grad():\n",
    "        for s in range(len(schedular.timesteps)):\n",
    "            if s>=start_step:\n",
    "                t=schedular.timesteps[s]\n",
    "                inp=torch.concat([img_latent,img_latent])\n",
    "                inp=schedular.scale_model_input(inp,t)\n",
    "\n",
    "                noise=unet(inp,t,encoder_hidden_states=token_emb).sample\n",
    "\n",
    "                pred_noise=noise[0]+hparam.guide_factor*(noise[1]-noise[0])\n",
    "                # pred_noise=pred_noise/pred_noise.norm()*noise[0].norm()\n",
    "                img_latent=schedular.step(pred_noise,t,img_latent).prev_sample\n",
    "            pbar.update(1)\n",
    "    return latents2Pil(img_latent),img_latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbee21-cb3a-4be5-b225-ca4c645f4925",
   "metadata": {},
   "source": [
    "## 重点先测试这2个函数是否好用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fd6bd-14b3-4028-833a-564ca6f51aa4",
   "metadata": {},
   "source": [
    "### 观察1\n",
    "固定noise,没有任何interp，树的形态没有发生大的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PYkOzs5zaFlu",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,1,0)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70472758-43f0-4dea-8401-b2ec28be4c25",
   "metadata": {},
   "source": [
    "![tree](imgs/tree1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6404789-9782-47b0-abb8-5b49b66dd91b",
   "metadata": {},
   "source": [
    "### 观察2\n",
    "固定noise,interp比例为0.8,0.2.树的形态没变，可以看到过度的很平缓自然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NPlrAedRbe8r",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,0.8,0.2)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87eeda-599b-43b3-99b6-3201710b216e",
   "metadata": {},
   "source": [
    "![tree](imgs/tree2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7d26b-9df7-4594-92aa-4e29da9aaa1c",
   "metadata": {},
   "source": [
    "### 观察3\n",
    "固定noise,interp比例为0.5,0.5，结论同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2NqZbbmWbtTi",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,0.5,0.5)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31b7b1-f792-4b9e-a5ce-5faafd3a4ee6",
   "metadata": {},
   "source": [
    "![tree](imgs/tree3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c87b9-4223-4fb9-9a6c-3ea6364361a6",
   "metadata": {},
   "source": [
    "### 观察4\n",
    "不固定noise,interp比例为0.5,0.5，树的形态发生改变，看来不是我想要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BvieRR7ocGIA",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=-1\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,0.5,0.5)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2c8fa-2363-4e65-b4ed-9c978945e873",
   "metadata": {},
   "source": [
    "![tree](imgs/tree4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f5100-ca91-445d-b71b-ab34858f8d25",
   "metadata": {},
   "source": [
    "### 观察5\n",
    "固定noise,interp比例为0.5,0.5，start_step设置20，变化太少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kVtmWhWUdtYt",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=20\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,0.5,0.5)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58fab3-9fc6-4bad-9900-3be80a6f1e8f",
   "metadata": {},
   "source": [
    "![tree](imgs/tree5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3119af3-31cf-4af2-8889-d7c1f59656f9",
   "metadata": {},
   "source": [
    "### 观察5.1\n",
    "固定noise,interp比例为0.5,0.5，start_step设置0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ba165-894b-47f3-9357-9bdc09ed2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=0\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,0.5,0.5)\n",
    "  frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "  gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70744a58-e4f9-4fcf-84cb-7df74f0ac0c8",
   "metadata": {},
   "source": [
    "### 异常6\n",
    "使用上一帧作为输入，观察退化现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb20e05-e459-48c4-8954-f43e7285bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "prev_latent=img_latent.clone()\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,1,0)\n",
    "  frame,prev_latent=generate_baseon_prompt_emb(prev_latent,tx)\n",
    "  gens.append(frame)\n",
    "  # hparam.start_step=+0\n",
    "  # if i==2:break\n",
    "image_grid(gens, 1, len(tree_prompts)).resize((7*256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a7c7d-a1f5-497e-8084-95c4bf2465d7",
   "metadata": {},
   "source": [
    "![tree](imgs/tree6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecd165-72c2-4709-ade6-6e414bc6c924",
   "metadata": {},
   "source": [
    "### 观察 7 (改进实验)\n",
    "使用上一帧作为输入，先生成一个初始化latent(prev_latent)，之后的训练start_step从0开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a46331-e85c-4873-be9f-cebcd014546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "prev_latent=img_latent.clone()\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "  tx=mix_frame_emb(i,i+1,1,0)\n",
    "  frame,prev_latent=generate_baseon_prompt_emb(prev_latent,tx)\n",
    "  gens.append(frame)\n",
    "  hparam.start_step=+0\n",
    "image_grid(gens, 1, len(tree_prompts)).resize((7*256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8da90-4669-47a4-889e-f817c3191072",
   "metadata": {},
   "source": [
    "### 观察 8 \n",
    "使用初始化种子latent，两个提示词直接的过度情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b38d5-c77b-445d-affc-d3453753ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "\n",
    "_,prev_latent=generate_baseon_prompt_emb(img_latent,mix_frame_emb(0,1,1,0))\n",
    "hparam.start_step=0\n",
    "\n",
    "rows,cols=2,8\n",
    "for r in range(1,rows+1):\n",
    "    w=1\n",
    "    for c in range(cols):\n",
    "\n",
    "        tx=mix_frame_emb(r,r+1,w,1-w)\n",
    "        w-=1/cols\n",
    "        frame,prev_latent=generate_baseon_prompt_emb(prev_latent,tx)\n",
    "        gens.append(frame)\n",
    "\n",
    "  \n",
    "image_grid(gens, rows, cols).resize((cols*256,rows*256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6281800-88a9-47b1-9016-810882435de3",
   "metadata": {},
   "source": [
    "### 观察 9\n",
    "如果全部start_step 0开始训练，每次都生成一样的图片吗？\n",
    "\n",
    "answer:当然不会，因为text_emb每次输入的是不同的。我预计应该是与原图的风格变化比较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2332fb-17e9-49a9-a8c9-a6ca563edff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=0\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "gens=[]\n",
    "for i in range(len(tree_prompts)):\n",
    "\n",
    "    tx=mix_frame_emb(i,i+1,0,1)\n",
    "    frame,frame_latent=generate_baseon_prompt_emb(img_latent,tx)\n",
    "    gens.append(frame)\n",
    "image_grid(gens, 1, len(tree_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7e160-fd10-4985-be0a-1f3a44ad6200",
   "metadata": {},
   "source": [
    "生成每一帧的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda70f3-0c40-4a9a-9683-19fb8007ab50",
   "metadata": {},
   "source": [
    "## 生成video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00cd86-5b76-4aa5-8335-da479442e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {hparam.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd4f53-fbbc-4888-a595-43f2c5b36f1a",
   "metadata": {},
   "source": [
    "### 方法1，固定初始latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f42e75-4bb9-4a27-9440-b6b0511c322c",
   "metadata": {},
   "source": [
    "生成30s的视频，每个提示词5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d6511-f351-48b0-9cd3-e07a6c3db2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84608e7da308499da9b7bef8aca2cec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.fps=10\n",
    "hparam.prompt_per_second=5\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "\n",
    "os.makedirs(hparam.output_dir,exist_ok=True)\n",
    "\n",
    "num_of_prompt=len(hparam.prompts)\n",
    "num_of_generate_frame=(num_of_prompt-1)*hparam.prompt_per_second*hparam.fps\n",
    "pbar=tqdm(range(num_of_generate_frame*hparam.num_inference))\n",
    "\n",
    "\n",
    "\n",
    "def mapfunc(t):\n",
    "    '''\n",
    "        把每一帧映射到对应的prompt索引\n",
    "    '''\n",
    "    frac_index=t*(num_of_prompt-1)/(num_of_generate_frame-1)\n",
    "    index1,index2=math.floor(frac_index),math.floor(frac_index)+1\n",
    "    weight1,weight2=index2-frac_index,frac_index-index1\n",
    "    \n",
    "    if index2>num_of_prompt-1:\n",
    "        index2=0\n",
    "    return index1,index2,weight1,weight2    \n",
    "\n",
    "#  (0, 1, 0.2941176470588235, 0.7058823529411765)\n",
    "for k,e in enumerate(map(mapfunc,range(num_of_generate_frame))):\n",
    "    pbar.set_description(f'Frame {k:05d}')\n",
    "    text_emb=mix_frame_emb(*e)\n",
    "    frame,frame_latent=generate_baseon_prompt_emb(img_latent,text_emb,pbar)\n",
    "    # img_latent=frame_latent\n",
    "    frame.save(f\"{hparam.output_dir}/{k:05d}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c6d20-35db-4ad3-b811-8c2dedb8f1b7",
   "metadata": {},
   "source": [
    "### 方法2，使用前一帧的latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0f43d-37b2-4f99-b7fa-03493cad0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam.num_inference=50\n",
    "hparam.start_step=10\n",
    "hparam.fps=10\n",
    "hparam.prompt_per_second=5\n",
    "hparam.seed=17390125398225616219\n",
    "\n",
    "\n",
    "os.makedirs(hparam.output_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "init_prompt,next_prompts=hparam.prompts[0],hparam.prompts[1:]\n",
    "num_of_prompt=len(next_prompts)\n",
    "num_of_generate_frame=(num_of_prompt-1)*hparam.prompt_per_second*hparam.fps\n",
    "pbar=tqdm(range(num_of_generate_frame*hparam.num_inference+hparam.num_inference))\n",
    "\n",
    "#初始化第一帧\n",
    "_,frame_latent=generate_baseon_prompt_emb(img_latent,mix_frame_emb(0,1,1,0),pbar)\n",
    "\n",
    "def mapfunc(t):\n",
    "    '''\n",
    "        把每一帧映射到对应的prompt索引,因为mix_frame_emb[0]不会被使用，所以需要 shift+1\n",
    "        \n",
    "        mix_frame_emb: [0,1,2,3,4,5,6]\n",
    "          current_emb:   [0,1,2,3,4,5]\n",
    "          \n",
    "          current_emb[0]-->mix_frame_emb[1]\n",
    "          current_emb[1]-->mix_frame_emb[2]\n",
    "                  ....\n",
    "          current_emb[5]-->mix_frame_emb[6]\n",
    "        \n",
    "    '''\n",
    "    frac_index=t*(num_of_prompt-1)/(num_of_generate_frame-1)\n",
    "    index1,index2=math.floor(frac_index),math.floor(frac_index)+1\n",
    "    weight1,weight2=index2-frac_index,frac_index-index1\n",
    "    \n",
    "    if index2>num_of_prompt-1:\n",
    "        index2=0\n",
    "    return index1+1,index2+1,weight1,weight2  \n",
    "\n",
    "hparam.start_step=0\n",
    "for k,e in enumerate(map(mapfunc,range(num_of_generate_frame))):\n",
    "    pbar.set_description(f'Frame {k:05d}')\n",
    "    text_emb=mix_frame_emb(*e)\n",
    "    frame,frame_latent=generate_baseon_prompt_emb(frame_latent,text_emb,pbar)\n",
    "    # img_latent=frame_latent\n",
    "    frame.save(f\"{hparam.output_dir}/{k:05d}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whV-ByUPeq2J",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "\n",
    "def create_movie(movie_name):\n",
    "    if os.path.exists(movie_name): os.remove(movie_name)\n",
    "    !ffmpeg -v 1 -y -f image2 -framerate {hparam.fps} -i {hparam.output_dir}/%05d.jpg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p {movie_name}\n",
    "# create_movie('grow_tree.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2gpA84Yl6T0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=600 controls>\n",
       "      <source src=\"imgs/grow_tree.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" %'imgs/grow_tree.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4Ke1T08m2M8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "264d173bf0e0447daac54056c5724884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_827c7e93b55941fd98ef9c106e331896",
      "max": 15000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2b09e2d3b9b4a9e94d5458fd2150f69",
      "value": 15000
     }
    },
    "329b2637321f47b393b2b05dec270ab4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "404a934427fa4003b44916141c9a49cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4166ae341c4248c3b4c819b1c179e73a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_404a934427fa4003b44916141c9a49cf",
      "placeholder": "​",
      "style": "IPY_MODEL_329b2637321f47b393b2b05dec270ab4",
      "value": "Frame 00299: "
     }
    },
    "827c7e93b55941fd98ef9c106e331896": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84608e7da308499da9b7bef8aca2cec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4166ae341c4248c3b4c819b1c179e73a",
       "IPY_MODEL_264d173bf0e0447daac54056c5724884",
       "IPY_MODEL_e89ee306d68843018ebec98e0c5648be"
      ],
      "layout": "IPY_MODEL_aaf3e146c2b4450fb3d44a23f4cd99b9"
     }
    },
    "aaf3e146c2b4450fb3d44a23f4cd99b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abfba4114d334380b5eace550ed91d9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20e82131c014693a5e8b012eb51fc46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2b09e2d3b9b4a9e94d5458fd2150f69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e89ee306d68843018ebec98e0c5648be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abfba4114d334380b5eace550ed91d9d",
      "placeholder": "​",
      "style": "IPY_MODEL_b20e82131c014693a5e8b012eb51fc46",
      "value": " 15300/? [32:55&lt;00:00,  6.46it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
